# Install necessary libraries
!pip install matplotlib seaborn scikit-learn
# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import BayesianRidge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from google.colab import files
# Upload a CSV file if you have one
print("Do you have a CSV file to upload? (yes/no)")
response = input().lower()

if response == "yes":
    # Upload the CSV file
    uploaded = files.upload()
    filename = list(uploaded.keys())[0]
else:
    # Generate synthetic data for demonstration
    np.random.seed(42) # Indented this line to be part of the 'else' block
    X = np.random.rand(100, 1) * 10  # Random data between 0 and 10
    y = 2 * X + 1 + np.random.randn(100, 1) * 2  # y = 2x + 1 with some noise

    # Convert to a DataFrame
    data = pd.DataFrame(np.hstack((X, y)), columns=["X", "y"])

    # Save to CSV for convenience
    filename = "synthetic_data.csv"
data.to_csv(filename, index=False)
print(f"Synthetic dataset saved as {filename}.")
# Load the dataset (for CSV file)
data = pd.read_csv(filename)

# Display first few rows
print("Dataset Preview:")
print(data.head())
# Separate features (X) and target (y)
X = data["X"].values.reshape(-1, 1)  # Feature matrix
y = data["y"].values  # Target vector

# Split the dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize the BayesianRidge model (which implements Bayesian Linear Regression)
bayesian_regressor = BayesianRidge()

# Fit the model on the training data
bayesian_regressor.fit(X_train, y_train)

# Predict on the test data
y_pred = bayesian_regressor.predict(X_test)
# Plot the prior and posterior distributions of the parameters
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Plot prior distribution (assuming the model starts with a standard prior)
ax[0].set_title("Prior Distribution (Assumed)")
ax[0].hist(np.random.normal(0, 1, 1000), bins=50, alpha=0.7, color='blue', label="Prior")
ax[0].legend()

# Plot posterior distribution (after model fitting)
ax[1].set_title("Posterior Distribution (After Fitting)")
ax[1].hist(bayesian_regressor.coef_, bins=50, alpha=0.7, color='green', label="Posterior")
ax[1].legend()

plt.show()
# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse:.2f}")
# Plot the true values and the predicted values
plt.figure(figsize=(8, 6))
plt.scatter(X_test, y_test, color="blue", label="True values")
plt.plot(X_test, y_pred, color="red", label="Predicted values", linewidth=2)
plt.title("Bayesian Linear Regression: True vs Predicted Values")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.show()
